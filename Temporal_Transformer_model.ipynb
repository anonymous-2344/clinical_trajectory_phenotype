{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd32e1e6",
   "metadata": {},
   "source": [
    "# Remove admissions with los<=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "176ef460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original admissions: 1777\n",
      "Remaining admissions (LOS > 2): 1660\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load admissions file\n",
    "df = pd.read_excel(\"final_admissions.xlsx\")\n",
    "# Drop admissions with LOS <= 2\n",
    "df_filtered = df[df[\"LOS\"] > 2].reset_index(drop=True)\n",
    "# Save filtered file\n",
    "df_filtered.to_excel(\"final_admissions1.xlsx\", index=False)\n",
    "print(f\"Original admissions: {len(df)}\")\n",
    "print(f\"Remaining admissions (LOS > 2): {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "79d35229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579b7b2",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "485de2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.read_excel(\"with_note_embedding/final-Q&A_based_Note_embeddings.xlsx\")\n",
    "adm_df = pd.read_excel(\"final_admissions.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a2d609e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Note_id</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176176_Day_2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...</td>\n",
       "      <td>176176</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176176_Day_3</td>\n",
       "      <td>[-1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 0, ...</td>\n",
       "      <td>176176</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185910_Day_1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...</td>\n",
       "      <td>185910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185910_Day_2</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, -1, 0, ...</td>\n",
       "      <td>185910</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185910_Day_3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0...</td>\n",
       "      <td>185910</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Note_id                                          Embedding  HADM_ID  \\\n",
       "0  176176_Day_2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...   176176   \n",
       "1  176176_Day_3  [-1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 0, ...   176176   \n",
       "2  185910_Day_1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...   185910   \n",
       "3  185910_Day_2  [0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, -1, 0, ...   185910   \n",
       "4  185910_Day_3  [0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0...   185910   \n",
       "\n",
       "   DAY  \n",
       "0    2  \n",
       "1    3  \n",
       "2    1  \n",
       "3    2  \n",
       "4    3  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "798ef3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "      <th>MORTALITY_STATUS</th>\n",
       "      <th>LOS</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>176176</td>\n",
       "      <td>2116-12-23 22:30:00</td>\n",
       "      <td>2116-12-27 12:05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>SEPSIS;TELEMETRY</td>\n",
       "      <td>82</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>185910</td>\n",
       "      <td>2166-08-10 00:28:00</td>\n",
       "      <td>2166-09-04 11:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>ACUTE MYOCARDIAL INFARCTION-SEPSIS</td>\n",
       "      <td>76</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>357</td>\n",
       "      <td>122609</td>\n",
       "      <td>2198-11-01 22:36:00</td>\n",
       "      <td>2198-11-14 14:20:00</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>SEPSIS</td>\n",
       "      <td>64</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>366</td>\n",
       "      <td>134462</td>\n",
       "      <td>2164-11-18 20:27:00</td>\n",
       "      <td>2164-11-22 15:18:00</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>SEPSIS</td>\n",
       "      <td>53</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>116009</td>\n",
       "      <td>2113-02-15 00:19:00</td>\n",
       "      <td>2113-02-19 15:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>SEPSIS,URINARY TRACT INFECTION</td>\n",
       "      <td>69</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID            ADMITTIME            DISCHTIME  \\\n",
       "0          33   176176  2116-12-23 22:30:00  2116-12-27 12:05:00   \n",
       "1          38   185910  2166-08-10 00:28:00  2166-09-04 11:30:00   \n",
       "2         357   122609  2198-11-01 22:36:00  2198-11-14 14:20:00   \n",
       "3         366   134462  2164-11-18 20:27:00  2164-11-22 15:18:00   \n",
       "4          62   116009  2113-02-15 00:19:00  2113-02-19 15:30:00   \n",
       "\n",
       "   MORTALITY_STATUS  LOS                           DIAGNOSIS  AGE GENDER  \n",
       "0                 0    5                    SEPSIS;TELEMETRY   82      M  \n",
       "1                 0   26  ACUTE MYOCARDIAL INFARCTION-SEPSIS   76      M  \n",
       "2                 0   14                              SEPSIS   64      M  \n",
       "3                 0    5                              SEPSIS   53      M  \n",
       "4                 0    5      SEPSIS,URINARY TRACT INFECTION   69      M  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e6fe2",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06d27dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df[\"embedding\"] = emb_df[\"Embedding\"].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5f60f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2424\n"
     ]
    }
   ],
   "source": [
    "EMB_DIM = len(emb_df[\"embedding\"].iloc[0])\n",
    "print(EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "414ac35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_groups = emb_df.groupby(\"HADM_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "458f9ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Note_id</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>DAY</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176176_Day_2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...</td>\n",
       "      <td>176176</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176176_Day_3</td>\n",
       "      <td>[-1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 0, ...</td>\n",
       "      <td>176176</td>\n",
       "      <td>3</td>\n",
       "      <td>[-1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185910_Day_1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...</td>\n",
       "      <td>185910</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185910_Day_2</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, -1, 0, ...</td>\n",
       "      <td>185910</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, -1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185910_Day_3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0...</td>\n",
       "      <td>185910</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8461</th>\n",
       "      <td>153703_Day_1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>153703</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8462</th>\n",
       "      <td>153703_Day_2</td>\n",
       "      <td>[1, -1, 1, 0, 0, 0, -1, 0, -1, 0, 1, 0, 1, 1, ...</td>\n",
       "      <td>153703</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, -1, 1, 0, 0, 0, -1, 0, -1, 0, 1, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8463</th>\n",
       "      <td>153703_Day_3</td>\n",
       "      <td>[0, -1, 1, 0, 0, 0, -1, 0, -1, 0, 1, 0, 1, -1,...</td>\n",
       "      <td>153703</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, -1, 1, 0, 0, 0, -1, 0, -1, 0, 1, 0, 1, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8464</th>\n",
       "      <td>153703_Day_4</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, -1, ...</td>\n",
       "      <td>153703</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8465</th>\n",
       "      <td>153703_Day_5</td>\n",
       "      <td>[0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, -1, ...</td>\n",
       "      <td>153703</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, -1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4361 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Note_id                                          Embedding  \\\n",
       "0     176176_Day_2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...   \n",
       "1     176176_Day_3  [-1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 0, ...   \n",
       "2     185910_Day_1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...   \n",
       "3     185910_Day_2  [0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, -1, 0, ...   \n",
       "4     185910_Day_3  [0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0...   \n",
       "...            ...                                                ...   \n",
       "8461  153703_Day_1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "8462  153703_Day_2  [1, -1, 1, 0, 0, 0, -1, 0, -1, 0, 1, 0, 1, 1, ...   \n",
       "8463  153703_Day_3  [0, -1, 1, 0, 0, 0, -1, 0, -1, 0, 1, 0, 1, -1,...   \n",
       "8464  153703_Day_4  [1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, -1, ...   \n",
       "8465  153703_Day_5  [0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, -1, ...   \n",
       "\n",
       "      HADM_ID  DAY                                          embedding  \n",
       "0      176176    2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...  \n",
       "1      176176    3  [-1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 0, ...  \n",
       "2      185910    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...  \n",
       "3      185910    2  [0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, -1, 0, ...  \n",
       "4      185910    3  [0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0...  \n",
       "...       ...  ...                                                ...  \n",
       "8461   153703    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "8462   153703    2  [1, -1, 1, 0, 0, 0, -1, 0, -1, 0, 1, 0, 1, 1, ...  \n",
       "8463   153703    3  [0, -1, 1, 0, 0, 0, -1, 0, -1, 0, 1, 0, 1, -1,...  \n",
       "8464   153703    4  [1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, -1, ...  \n",
       "8465   153703    5  [0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, -1, ...  \n",
       "\n",
       "[4361 rows x 5 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_groups.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c19b980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_admission_sequence(hadm_id, emb_group, los, emb_dim):\n",
    "   \"\"\"\n",
    "   Returns:\n",
    "     embeddings: (T, D)\n",
    "     deltas: (T,)\n",
    "     mask: (T,)  -> 1 if observed, 0 if missing\n",
    "   \"\"\"\n",
    "   day_to_emb = {\n",
    "       row.DAY: np.array(row.embedding)\n",
    "       for _, row in emb_group.iterrows()\n",
    "   }\n",
    "   embeddings = []\n",
    "   deltas = []\n",
    "   mask = []\n",
    "   prev_day = None\n",
    "   for day in range(1, los + 1):\n",
    "       if day in day_to_emb:\n",
    "           emb = day_to_emb[day]\n",
    "           m = 1\n",
    "       else:\n",
    "           emb = np.zeros(emb_dim)\n",
    "           m = 0\n",
    "       if prev_day is None:\n",
    "           delta = 0\n",
    "       else:\n",
    "           delta = day - prev_day\n",
    "       embeddings.append(emb)\n",
    "       deltas.append(delta)\n",
    "       mask.append(m)\n",
    "       if m == 1:\n",
    "           prev_day = day\n",
    "   return (\n",
    "       np.stack(embeddings),\n",
    "       np.array(deltas),\n",
    "       np.array(mask)\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5896257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "admission_data = {}\n",
    "for _, row in adm_df.iterrows():\n",
    "   hadm_id = row.HADM_ID\n",
    "   los = int(row.LOS)\n",
    "   if hadm_id not in emb_groups.groups:\n",
    "       continue\n",
    "   emb_group = emb_groups.get_group(hadm_id)\n",
    "   emb, delta, mask = build_admission_sequence(\n",
    "       hadm_id, emb_group, los, EMB_DIM\n",
    "   )\n",
    "   admission_data[hadm_id] = {\n",
    "       \"HADM_ID\": hadm_id,\n",
    "       \"emb\": emb,\n",
    "       \"delta\": delta,\n",
    "       \"mask\": mask\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88cdf4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(admission_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f8553d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HADM_ID': 199855,\n",
       " 'emb': array([[ 0., -1.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " 'delta': array([0, 1, 1, 1, 2, 3]),\n",
       " 'mask': array([1, 1, 1, 0, 0, 0])}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admission_data[199855]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2334b80",
   "metadata": {},
   "source": [
    "# Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a3936cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "510f1447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968 243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hadm_ids = list(admission_data.keys())\n",
    "train_ids, val_ids = train_test_split(\n",
    "   hadm_ids,\n",
    "   test_size=0.2,\n",
    "   random_state=42\n",
    ")\n",
    "train_data = {k: admission_data[k] for k in train_ids}\n",
    "val_data   = {k: admission_data[k] for k in val_ids}\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "abd0249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdmissionTrainDataset(Dataset):\n",
    "   def __init__(self, data, max_prefix=3):\n",
    "       self.data = list(data.values())\n",
    "       self.max_prefix = max_prefix\n",
    "   def __len__(self):\n",
    "       return len(self.data)\n",
    "   def __getitem__(self, idx):\n",
    "       item = self.data[idx]\n",
    "       emb = torch.tensor(item[\"emb\"], dtype=torch.float)\n",
    "       delta = torch.tensor(item[\"delta\"], dtype=torch.float)\n",
    "       mask = torch.tensor(item[\"mask\"], dtype=torch.float)\n",
    "       T = emb.shape[0]\n",
    "       max_k = min(self.max_prefix, T)\n",
    "       # -----------------------------\n",
    "       # CRITICAL FIX: mask-aware prefix\n",
    "       # -----------------------------\n",
    "       valid_idx = torch.where(mask == 1)[0]\n",
    "       if len(valid_idx) == 0:\n",
    "           raise RuntimeError(\"Admission has no valid observed days\")\n",
    "       # Case 1: at least one valid day occurs within max_prefix\n",
    "       valid_within = valid_idx[valid_idx < max_k]\n",
    "       if len(valid_within) > 0:\n",
    "           last_valid_pos = valid_within.max().item()\n",
    "           k = torch.randint(\n",
    "               low=last_valid_pos + 1,\n",
    "               high=max_k + 1,\n",
    "               size=(1,)\n",
    "           ).item()\n",
    "       else:\n",
    "           # Case 2: first valid day occurs after max_prefix\n",
    "           # Extend prefix to include first valid observation\n",
    "           first_valid_pos = valid_idx.min().item()\n",
    "           k = first_valid_pos + 1\n",
    "       prefix_emb = emb[:k]\n",
    "       prefix_delta = delta[:k]\n",
    "       prefix_mask = mask[:k]\n",
    "       # FINAL SAFETY CHECK\n",
    "       assert prefix_mask.sum() > 0, \"Prefix is fully masked â€” this should never happen\"\n",
    "       return {\n",
    "           \"hadm_id\": item[\"HADM_ID\"],\n",
    "           \"full_emb\": emb,\n",
    "           \"full_delta\": delta,\n",
    "           \"full_mask\": mask,\n",
    "           \"prefix_emb\": prefix_emb,\n",
    "           \"prefix_delta\": prefix_delta,\n",
    "           \"prefix_mask\": prefix_mask,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7ae6e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdmissionValDataset(Dataset):\n",
    "   def __init__(self, data, prefix_len=2):\n",
    "       self.data = list(data.values())\n",
    "       self.prefix_len = prefix_len\n",
    "   def __len__(self):\n",
    "       return len(self.data)\n",
    "   def __getitem__(self, idx):\n",
    "       item = self.data[idx]\n",
    "       emb = torch.tensor(item[\"emb\"], dtype=torch.float)\n",
    "       delta = torch.tensor(item[\"delta\"], dtype=torch.float)\n",
    "       mask = torch.tensor(item[\"mask\"], dtype=torch.float)\n",
    "       T = emb.shape[0]\n",
    "       k = min(self.prefix_len, T)\n",
    "       # -------- FIX: ensure prefix has â‰¥1 valid timestep --------\n",
    "       if mask[:k].sum() == 0:\n",
    "           valid_idx = torch.where(mask == 1)[0]\n",
    "           if len(valid_idx) == 0:\n",
    "               raise RuntimeError(\"Admission has no valid observed days\")\n",
    "           k = valid_idx.min().item() + 1\n",
    "       prefix_emb = emb[:k]\n",
    "       prefix_delta = delta[:k]\n",
    "       prefix_mask = mask[:k]\n",
    "       # Safety check (can remove later)\n",
    "       assert prefix_mask.sum() > 0, \"Validation prefix is fully masked\"\n",
    "       return {\n",
    "           \"hadm_id\": item[\"HADM_ID\"],\n",
    "           \"full_emb\": emb,\n",
    "           \"full_delta\": delta,\n",
    "           \"full_mask\": mask,\n",
    "           \"prefix_emb\": prefix_emb,\n",
    "           \"prefix_delta\": prefix_delta,\n",
    "           \"prefix_mask\": prefix_mask,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "83fdd953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "   def pad(key, padding_value=0.0):\n",
    "       return pad_sequence(\n",
    "           [b[key] for b in batch],\n",
    "           batch_first=True,\n",
    "           padding_value=padding_value\n",
    "       )\n",
    "   out = {\n",
    "       \"hadm_id\": [b[\"hadm_id\"] for b in batch],\n",
    "       \"full_emb\": pad(\"full_emb\", 0.0),\n",
    "       \"full_delta\": pad(\"full_delta\", 0.0),\n",
    "       \"full_mask\": pad(\"full_mask\", 0.0),\n",
    "       \"prefix_emb\": pad(\"prefix_emb\", 0.0),\n",
    "       \"prefix_delta\": pad(\"prefix_delta\", 0.0),\n",
    "       \"prefix_mask\": pad(\"prefix_mask\", 0.0),\n",
    "   }\n",
    "   # -------- sanity checks --------\n",
    "   assert (out[\"full_mask\"].sum(dim=1) > 0).all(), \\\n",
    "       \"Found full sequence with all-masked timesteps\"\n",
    "   assert (out[\"prefix_mask\"].sum(dim=1) > 0).all(), \\\n",
    "       \"Found prefix with all-masked timesteps\"\n",
    "   return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "660fa941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(\n",
    "   AdmissionTrainDataset(train_data),\n",
    "   batch_size=16,\n",
    "   shuffle=True,\n",
    "   collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "   AdmissionValDataset(val_data),\n",
    "   batch_size=16,\n",
    "   shuffle=False,\n",
    "   collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "63feb802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "   def __init__(self, dim):\n",
    "       super().__init__()\n",
    "       self.net = nn.Sequential(\n",
    "           nn.Linear(1, dim),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(dim, dim)\n",
    "       )\n",
    "   def forward(self, delta):\n",
    "       return self.net(delta.unsqueeze(-1))\n",
    "\n",
    "class MaskEmbedding(nn.Module):\n",
    "   def __init__(self, dim):\n",
    "       super().__init__()\n",
    "       self.emb = nn.Embedding(2, dim)\n",
    "   def forward(self, mask):\n",
    "       return self.emb(mask.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dc11990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalTransformer(nn.Module):\n",
    "   def __init__(self, dim, n_heads=4, n_layers=4):\n",
    "       super().__init__()\n",
    "       self.time_emb = TimeEmbedding(dim)\n",
    "       self.mask_emb = MaskEmbedding(dim)\n",
    "       encoder_layer = nn.TransformerEncoderLayer(\n",
    "           d_model=dim,\n",
    "           nhead=n_heads,\n",
    "           batch_first=True\n",
    "       )\n",
    "       self.encoder = nn.TransformerEncoder(\n",
    "           encoder_layer,\n",
    "           num_layers=n_layers\n",
    "       )\n",
    "       self.query = nn.Parameter(torch.randn(dim))\n",
    "   def forward(self, emb, delta, mask):\n",
    "       \"\"\"\n",
    "       emb:   (B, T, D)\n",
    "       delta: (B, T)\n",
    "       mask:  (B, T)   with values {0,1}\n",
    "       \"\"\"\n",
    "       x = emb + self.time_emb(delta) + self.mask_emb(mask)\n",
    "       key_padding_mask = (mask == 0)\n",
    "       h = self.encoder(\n",
    "           x,\n",
    "           src_key_padding_mask=key_padding_mask\n",
    "       )  # (B, T, D)\n",
    "       # -------- FIX: mask-aware attention pooling --------\n",
    "       scores = torch.matmul(h, self.query)          # (B, T)\n",
    "       scores = scores.masked_fill(mask == 0, -1e9)  # mask padded steps\n",
    "       attn = torch.softmax(scores, dim=1)\n",
    "       attn = attn * mask                             # zero out padded\n",
    "       attn = attn / (attn.sum(dim=1, keepdim=True) + 1e-8)\n",
    "       adm_emb = torch.sum(h * attn.unsqueeze(-1), dim=1)\n",
    "       return adm_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b8a81",
   "metadata": {},
   "source": [
    "# prefix-to-full Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a08a4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contrastive_loss(x, y, temperature=0.1):\n",
    "   \"\"\"\n",
    "   x, y: (B, D) embeddings\n",
    "   \"\"\"\n",
    "   x = F.normalize(x, dim=1)\n",
    "   y = F.normalize(y, dim=1)\n",
    "   logits = torch.matmul(x, y.T) / temperature\n",
    "   labels = torch.arange(x.size(0)).to(x.device)\n",
    "   return F.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab95c70",
   "metadata": {},
   "source": [
    "# Implement Temporal Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "18fbffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_day_drop(emb, delta, mask, drop_prob=0.2, min_len=3):\n",
    "   \"\"\"\n",
    "   emb:   (T, D)\n",
    "   delta: (T,)\n",
    "   mask:  (T,)  with values {0,1}\n",
    "   \"\"\"\n",
    "   # Only keep originally valid (observed) days\n",
    "   valid_indices = [i for i in range(len(mask)) if mask[i] == 1]\n",
    "   if len(valid_indices) < min_len:\n",
    "       # Extremely rare (LOS > 2), but safety first\n",
    "       valid_indices = valid_indices\n",
    "   keep_indices = []\n",
    "   for i in valid_indices:\n",
    "       if random.random() > drop_prob:\n",
    "           keep_indices.append(i)\n",
    "   # Enforce minimum length\n",
    "   if len(keep_indices) < min_len:\n",
    "       keep_indices = valid_indices[:min_len]\n",
    "   emb_new = emb[keep_indices]\n",
    "   delta_new = delta[keep_indices]\n",
    "   # ðŸ”‘ CRITICAL FIX: augmented mask must be all ones\n",
    "   mask_new = torch.ones(len(keep_indices), dtype=torch.float32, device=emb.device)\n",
    "   return emb_new, delta_new, mask_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e2e4dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter_delta(delta, noise_std=0.1):\n",
    "   noise = torch.randn_like(delta) * noise_std\n",
    "   delta_new = delta + noise\n",
    "   delta_new = torch.clamp(delta_new, min=0)\n",
    "   return delta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e4c19246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_augment(batch, drop_prob=0.2, min_len=3):\n",
    "   emb = batch[\"full_emb\"]     # (B, T, D)\n",
    "   delta = batch[\"full_delta\"] # (B, T)\n",
    "   mask = batch[\"full_mask\"]   # (B, T)\n",
    "   emb_aug, delta_aug, mask_aug = [], [], []\n",
    "   B = emb.size(0)\n",
    "   device = emb.device\n",
    "   for i in range(B):\n",
    "       e, d, m = random_day_drop(\n",
    "           emb[i], delta[i], mask[i],\n",
    "           drop_prob=drop_prob,\n",
    "           min_len=min_len\n",
    "       )\n",
    "       # Optional but recommended\n",
    "       d = jitter_delta(d)\n",
    "       emb_aug.append(e)\n",
    "       delta_aug.append(d)\n",
    "       mask_aug.append(m)\n",
    "   return emb_aug, delta_aug, mask_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9e444478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_augmented(emb_list, delta_list, mask_list):\n",
    "   emb_pad = pad_sequence(emb_list, batch_first=True, padding_value=0.0)\n",
    "   delta_pad = pad_sequence(delta_list, batch_first=True, padding_value=0.0)\n",
    "   mask_pad = pad_sequence(mask_list, batch_first=True, padding_value=0.0)\n",
    "   return emb_pad, delta_pad, mask_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603966b1",
   "metadata": {},
   "source": [
    "# Model traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "02249742",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TemporalTransformer(dim=EMB_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "73ec1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "lambda_aug = 0.2\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = -1\n",
    "checkpoint_path = \"best_temporal_transformer.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "655dc8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]:   0%|                                                                        | 0/61 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2424) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m         batch[k] \u001b[38;5;241m=\u001b[39m batch[k]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ----- Prefixâ€“Full loss -----\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m e_full \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m     16\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_emb\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     17\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_delta\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     18\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m e_prefix \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m     21\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix_emb\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     22\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix_delta\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     23\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m loss_pf \u001b[38;5;241m=\u001b[39m contrastive_loss(e_prefix, e_full)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[101], line 22\u001b[0m, in \u001b[0;36mTemporalTransformer.forward\u001b[1;34m(self, emb, delta, mask)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, emb, delta, mask):\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    emb:   (B, T, D)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    delta: (B, T)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    mask:  (B, T)   with values {0,1}\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m emb \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_emb(delta) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_emb(mask)\n\u001b[0;32m     23\u001b[0m     key_padding_mask \u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m     25\u001b[0m         x,\n\u001b[0;32m     26\u001b[0m         src_key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask\n\u001b[0;32m     27\u001b[0m     )  \u001b[38;5;66;03m# (B, T, D)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2424) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "   # =========================\n",
    "   # Training\n",
    "   # =========================\n",
    "   model.train()\n",
    "   train_loss_total = 0.0\n",
    "   train_pf_total = 0.0\n",
    "   train_aug_total = 0.0\n",
    "   train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\")\n",
    "   for batch in train_bar:\n",
    "       for k in batch:\n",
    "           if k != \"hadm_id\":\n",
    "               batch[k] = batch[k].to(device)\n",
    "       # ----- Prefixâ€“Full loss -----\n",
    "       e_full = model(\n",
    "           batch[\"full_emb\"],\n",
    "           batch[\"full_delta\"],\n",
    "           batch[\"full_mask\"]\n",
    "       )\n",
    "       e_prefix = model(\n",
    "           batch[\"prefix_emb\"],\n",
    "           batch[\"prefix_delta\"],\n",
    "           batch[\"prefix_mask\"]\n",
    "       )\n",
    "       loss_pf = contrastive_loss(e_prefix, e_full)\n",
    "       # ----- Temporal augmentation loss -----\n",
    "       emb_aug1, delta_aug1, mask_aug1 = temporal_augment(batch)\n",
    "       emb_aug2, delta_aug2, mask_aug2 = temporal_augment(batch)\n",
    "       emb_aug1, delta_aug1, mask_aug1 = pad_augmented(\n",
    "           emb_aug1, delta_aug1, mask_aug1\n",
    "       )\n",
    "       emb_aug2, delta_aug2, mask_aug2 = pad_augmented(\n",
    "           emb_aug2, delta_aug2, mask_aug2\n",
    "       )\n",
    "       emb_aug1 = emb_aug1.to(device)\n",
    "       delta_aug1 = delta_aug1.to(device)\n",
    "       mask_aug1 = mask_aug1.to(device)\n",
    "       emb_aug2 = emb_aug2.to(device)\n",
    "       delta_aug2 = delta_aug2.to(device)\n",
    "       mask_aug2 = mask_aug2.to(device)\n",
    "       e_aug1 = model(emb_aug1, delta_aug1, mask_aug1)\n",
    "       e_aug2 = model(emb_aug2, delta_aug2, mask_aug2)\n",
    "       loss_aug = contrastive_loss(e_aug1, e_aug2)\n",
    "       # ----- Total loss -----\n",
    "       loss = loss_pf + lambda_aug * loss_aug\n",
    "       optimizer.zero_grad()\n",
    "       loss.backward()\n",
    "       torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "       optimizer.step()\n",
    "       train_loss_total += loss.item()\n",
    "       train_pf_total += loss_pf.item()\n",
    "       train_aug_total += loss_aug.item()\n",
    "       train_bar.set_postfix({\n",
    "           \"loss\": f\"{loss.item():.4f}\",\n",
    "           \"pf\": f\"{loss_pf.item():.4f}\",\n",
    "           \"aug\": f\"{loss_aug.item():.4f}\"\n",
    "       })\n",
    "   avg_train_loss = train_loss_total / len(train_loader)\n",
    "   avg_train_pf = train_pf_total / len(train_loader)\n",
    "   avg_train_aug = train_aug_total / len(train_loader)\n",
    "   # =========================\n",
    "   # Validation\n",
    "   # =========================\n",
    "   model.eval()\n",
    "   val_loss_total = 0.0\n",
    "   with torch.no_grad():\n",
    "       for batch in val_loader:\n",
    "           for k in batch:\n",
    "               if k != \"hadm_id\":\n",
    "                   batch[k] = batch[k].to(device)\n",
    "           e_full = model(\n",
    "               batch[\"full_emb\"],\n",
    "               batch[\"full_delta\"],\n",
    "               batch[\"full_mask\"]\n",
    "           )\n",
    "           e_prefix = model(\n",
    "               batch[\"prefix_emb\"],\n",
    "               batch[\"prefix_delta\"],\n",
    "               batch[\"prefix_mask\"]\n",
    "           )\n",
    "           loss_val = contrastive_loss(e_prefix, e_full)\n",
    "           val_loss_total += loss_val.item()\n",
    "   avg_val_loss = val_loss_total / len(val_loader)\n",
    "   # =========================\n",
    "   # Save best checkpoint\n",
    "   # =========================\n",
    "   if avg_val_loss < best_val_loss:\n",
    "       best_val_loss = avg_val_loss\n",
    "       best_epoch = epoch\n",
    "       torch.save(\n",
    "           {\n",
    "               \"epoch\": epoch,\n",
    "               \"model_state_dict\": model.state_dict(),\n",
    "               \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "               \"val_loss\": avg_val_loss,\n",
    "           },\n",
    "           checkpoint_path\n",
    "       )\n",
    "       print(f\" Best model saved at epoch {epoch} (Val Loss = {avg_val_loss:.4f})\")\n",
    "   # =========================\n",
    "   # Epoch summary\n",
    "   # =========================\n",
    "   print(\n",
    "       f\"\\nEpoch {epoch}/{num_epochs} Summary\\n\"\n",
    "       f\"  Train Loss       : {avg_train_loss:.4f}\\n\"\n",
    "       f\"    â”œâ”€ Prefixâ€“Full : {avg_train_pf:.4f}\\n\"\n",
    "       f\"    â””â”€ Augmentation: {avg_train_aug:.4f}\\n\"\n",
    "       f\"  Val Loss         : {avg_val_loss:.4f}\\n\"\n",
    "       f\"{'-'*60}\"\n",
    "   )\n",
    "print(\n",
    "   f\"\\nTraining completed.\\n\"\n",
    "   f\"Best model from epoch {best_epoch} \"\n",
    "   f\"with validation loss {best_val_loss:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d807e3",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c8f725aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1632252\\AppData\\Local\\Temp\\ipykernel_6088\\1541525066.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3 (val loss = 0.0197)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TemporalTransformer(\n",
    "   dim=EMB_DIM,\n",
    "   n_heads=4,\n",
    "   n_layers=4\n",
    ").to(device)\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "print(\n",
    "   f\"Loaded model from epoch {checkpoint['epoch']} \"\n",
    "   f\"(val loss = {checkpoint['val_loss']:.4f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "19bf2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdmissionTestDataset(Dataset):\n",
    "   def __init__(self, data, prefix_len=2):\n",
    "       self.data = list(data.values())\n",
    "       self.prefix_len = prefix_len\n",
    "   def __len__(self):\n",
    "       return len(self.data)\n",
    "   def __getitem__(self, idx):\n",
    "       item = self.data[idx]\n",
    "       emb = torch.tensor(item[\"emb\"], dtype=torch.float)\n",
    "       delta = torch.tensor(item[\"delta\"], dtype=torch.float)\n",
    "       mask = torch.tensor(item[\"mask\"], dtype=torch.float)\n",
    "       T = emb.shape[0]\n",
    "       k = min(self.prefix_len, T)\n",
    "       # -------- FIX: ensure prefix has â‰¥1 valid timestep --------\n",
    "       if mask[:k].sum() == 0:\n",
    "           valid_idx = torch.where(mask == 1)[0]\n",
    "           if len(valid_idx) == 0:\n",
    "               raise RuntimeError(\"Admission has no valid observed days\")\n",
    "           k = valid_idx.min().item() + 1\n",
    "       prefix_emb = emb[:k]\n",
    "       prefix_delta = delta[:k]\n",
    "       prefix_mask = mask[:k]\n",
    "       # Safety check (can remove later)\n",
    "       assert prefix_mask.sum() > 0, \"Validation prefix is fully masked\"\n",
    "       return {\n",
    "           \"hadm_id\": item[\"HADM_ID\"],\n",
    "           \"full_emb\": emb,\n",
    "           \"full_delta\": delta,\n",
    "           \"full_mask\": mask,\n",
    "           \"prefix_emb\": prefix_emb,\n",
    "           \"prefix_delta\": prefix_delta,\n",
    "           \"prefix_mask\": prefix_mask,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2c6eab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_collate_fn(batch):\n",
    "#    return {\n",
    "#        \"hadm_id\": [b[\"hadm_id\"] for b in batch],\n",
    "#        \"emb\": pad_sequence([b[\"emb\"] for b in batch], batch_first=True, padding_value=0.0),\n",
    "#        \"delta\": pad_sequence([b[\"delta\"] for b in batch], batch_first=True, padding_value=0.0),\n",
    "#        \"mask\": pad_sequence([b[\"mask\"] for b in batch], batch_first=True, padding_value=0.0),\n",
    "#    }\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "   def pad(key, padding_value=0.0):\n",
    "       return pad_sequence(\n",
    "           [b[key] for b in batch],\n",
    "           batch_first=True,\n",
    "           padding_value=padding_value\n",
    "       )\n",
    "   out = {\n",
    "       \"hadm_id\": [b[\"hadm_id\"] for b in batch],\n",
    "       \"full_emb\": pad(\"full_emb\", 0.0),\n",
    "       \"full_delta\": pad(\"full_delta\", 0.0),\n",
    "       \"full_mask\": pad(\"full_mask\", 0.0),\n",
    "       \"prefix_emb\": pad(\"prefix_emb\", 0.0),\n",
    "       \"prefix_delta\": pad(\"prefix_delta\", 0.0),\n",
    "       \"prefix_mask\": pad(\"prefix_mask\", 0.0),\n",
    "   }\n",
    "   return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d1d15f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(admission_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5127de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = AdmissionTestDataset(admission_data)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "   test_dataset,\n",
    "   batch_size=32,\n",
    "   shuffle=False,\n",
    "   collate_fn=test_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1e97462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [02:55<00:00,  4.63s/it]\n"
     ]
    }
   ],
   "source": [
    "all_hadm_ids = []\n",
    "all_full_embeddings = []\n",
    "all_pref_embeddings = []\n",
    "with torch.no_grad():\n",
    "   for batch in tqdm(test_loader, desc=\"Test\"):\n",
    "       #print(batch)\n",
    "       full_emb = batch[\"full_emb\"].to(device)\n",
    "       full_delta = batch[\"full_delta\"].to(device)\n",
    "       full_mask = batch[\"full_mask\"].to(device)\n",
    "       full_adm_emb = model(full_emb, full_delta, full_mask)  # (B, D)\n",
    "       all_full_embeddings.append(full_adm_emb.cpu().numpy())\n",
    "       pref_emb = batch[\"prefix_emb\"].to(device)\n",
    "       pref_delta = batch[\"prefix_delta\"].to(device)\n",
    "       pref_mask = batch[\"prefix_mask\"].to(device)\n",
    "       pref_adm_emb = model(pref_emb, pref_delta, pref_mask)  # (B, D)\n",
    "       all_pref_embeddings.append(pref_adm_emb.cpu().numpy())\n",
    "       all_hadm_ids.extend(batch[\"hadm_id\"])\n",
    "all_full_embeddings = np.vstack(all_full_embeddings)\n",
    "all_pref_embeddings = np.vstack(all_pref_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "90d3a7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for 1211 admissions.\n"
     ]
    }
   ],
   "source": [
    "df_embeddings = pd.DataFrame({\n",
    "   \"HADM_ID\": all_hadm_ids,\n",
    "   \"full_admission_embedding\": all_full_embeddings.tolist()\n",
    "})\n",
    "df_embeddings.to_csv(\n",
    "   \"temporal_admission_embeddings.csv\",\n",
    "   index=False\n",
    ")\n",
    "print(f\"Saved embeddings for {len(df_embeddings)} admissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "59f6a589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2424"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_full_embeddings[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "936dc3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.4708498418331146,\n",
       " -0.5475208759307861,\n",
       " 0.28125613927841187,\n",
       " 0.516147792339325,\n",
       " 0.36070409417152405,\n",
       " -0.04363811016082764,\n",
       " 0.9618723392486572,\n",
       " 0.3940972089767456,\n",
       " 2.942169666290283,\n",
       " 1.1325111389160156,\n",
       " -0.9110435247421265,\n",
       " -0.5094019770622253,\n",
       " 0.06209389865398407,\n",
       " 0.30523550510406494,\n",
       " -0.14319802820682526,\n",
       " -1.0020883083343506,\n",
       " 0.07837782055139542,\n",
       " 1.7236154079437256,\n",
       " -0.23076598346233368,\n",
       " 0.04637850075960159,\n",
       " 0.5555213093757629,\n",
       " -0.9091050028800964,\n",
       " 0.34936612844467163,\n",
       " 0.40949851274490356,\n",
       " 0.5398333668708801,\n",
       " -0.6962534785270691,\n",
       " 1.7707552909851074,\n",
       " -0.9944214224815369,\n",
       " -0.9976385235786438,\n",
       " -0.011357879266142845,\n",
       " -0.3703729510307312,\n",
       " 0.12053392082452774,\n",
       " -0.17379771173000336,\n",
       " -0.2549597918987274,\n",
       " -0.5007861852645874,\n",
       " -2.7270724773406982,\n",
       " 0.7572588920593262,\n",
       " -1.2707879543304443,\n",
       " 1.1494656801223755,\n",
       " -1.1632790565490723,\n",
       " -0.7993175387382507,\n",
       " -0.724682629108429,\n",
       " -2.9457409381866455,\n",
       " -2.882647752761841,\n",
       " 0.8837493062019348,\n",
       " -0.16319385170936584,\n",
       " -0.5328891277313232,\n",
       " -0.5699352025985718,\n",
       " 0.17789143323898315,\n",
       " -0.14105936884880066,\n",
       " -0.6115202307701111,\n",
       " -0.2180282473564148,\n",
       " 2.2052316665649414,\n",
       " -0.49713218212127686,\n",
       " 0.09534981846809387,\n",
       " 0.5634851455688477,\n",
       " 0.34360170364379883,\n",
       " 0.5123326182365417,\n",
       " 0.3269582688808441,\n",
       " 0.31868305802345276,\n",
       " 0.16104312241077423,\n",
       " -1.0112333297729492,\n",
       " -1.0708134174346924,\n",
       " -1.1046606302261353,\n",
       " 0.904183566570282,\n",
       " -1.2519429922103882,\n",
       " -0.7084982395172119,\n",
       " -0.6918214559555054,\n",
       " 0.17042243480682373,\n",
       " -1.5335044860839844,\n",
       " 1.1282014846801758,\n",
       " 1.0668139457702637,\n",
       " 0.3021085858345032,\n",
       " 1.5229195356369019,\n",
       " 0.6005697250366211,\n",
       " 1.20163893699646,\n",
       " -0.5386826992034912,\n",
       " -0.46596384048461914,\n",
       " -1.6320054531097412,\n",
       " -1.0836405754089355,\n",
       " 0.31059831380844116,\n",
       " -0.32236042618751526,\n",
       " -0.3220781087875366,\n",
       " 1.22951340675354,\n",
       " -0.05892471969127655,\n",
       " 0.864080548286438,\n",
       " -0.24181850254535675,\n",
       " -0.1305660605430603,\n",
       " -0.0330747626721859,\n",
       " 0.13548125326633453,\n",
       " -0.06837725639343262,\n",
       " -0.4390771985054016,\n",
       " 0.21955884993076324,\n",
       " -0.05345050245523453,\n",
       " 1.4600584506988525,\n",
       " 1.0840439796447754,\n",
       " 0.7843953967094421,\n",
       " -0.22579388320446014,\n",
       " -1.2764705419540405,\n",
       " -0.8242698311805725,\n",
       " 0.21937990188598633,\n",
       " -1.0315204858779907,\n",
       " -0.6220580339431763,\n",
       " 0.9851937890052795,\n",
       " -1.0421628952026367,\n",
       " 1.0030903816223145,\n",
       " -0.4256157875061035,\n",
       " 2.505117893218994,\n",
       " -1.2107452154159546,\n",
       " -1.356902003288269,\n",
       " 0.5229510068893433,\n",
       " 0.8965339064598083,\n",
       " 0.2671564817428589,\n",
       " 0.7722269296646118,\n",
       " 0.2935998737812042,\n",
       " -0.08517471700906754,\n",
       " -0.18720164895057678,\n",
       " -1.4571459293365479,\n",
       " 1.0592910051345825,\n",
       " -2.3172953128814697,\n",
       " 0.09606591612100601,\n",
       " -0.41627955436706543,\n",
       " 1.3355422019958496,\n",
       " 0.6386972665786743,\n",
       " -0.29003921151161194,\n",
       " -0.7240203619003296,\n",
       " 1.1318113803863525,\n",
       " -0.8033247590065002,\n",
       " -1.5342532396316528,\n",
       " -0.30268561840057373,\n",
       " 1.4721218347549438,\n",
       " 0.4109630584716797,\n",
       " 0.5368886590003967,\n",
       " 1.3807815313339233,\n",
       " -0.6067203283309937,\n",
       " 0.01743142679333687,\n",
       " 0.6450781226158142,\n",
       " -1.022343397140503,\n",
       " -1.957176685333252,\n",
       " 0.3234896957874298,\n",
       " -0.31547290086746216,\n",
       " -0.25579899549484253,\n",
       " 1.205506682395935,\n",
       " 1.0376543998718262,\n",
       " 1.0492929220199585,\n",
       " 0.03981684520840645,\n",
       " -0.9251815676689148,\n",
       " 0.432638019323349,\n",
       " -1.1615973711013794,\n",
       " -0.07711201161146164,\n",
       " -0.18776771426200867,\n",
       " 1.101994276046753,\n",
       " -0.9130374193191528,\n",
       " 1.7815982103347778,\n",
       " -2.5196638107299805,\n",
       " -0.9744413495063782,\n",
       " -0.7971374988555908,\n",
       " -0.32347625494003296,\n",
       " 0.5143318772315979,\n",
       " -0.014976192265748978,\n",
       " 1.7688302993774414,\n",
       " 0.8765661120414734,\n",
       " -0.4384486973285675,\n",
       " -1.3979628086090088,\n",
       " -0.5359796285629272,\n",
       " 1.5940077304840088,\n",
       " -0.2538853883743286,\n",
       " -1.21024489402771,\n",
       " 0.07722434401512146,\n",
       " -1.653472661972046,\n",
       " -0.9018782377243042,\n",
       " 0.7944680452346802,\n",
       " -0.8486105799674988,\n",
       " 1.0017566680908203,\n",
       " 0.6583023071289062,\n",
       " 1.315531611442566,\n",
       " -0.06830734759569168,\n",
       " -0.059127070009708405,\n",
       " -1.6655795574188232,\n",
       " 0.1092396154999733,\n",
       " -1.992719292640686,\n",
       " 0.5977964401245117,\n",
       " -0.7780888676643372,\n",
       " 0.03500940650701523,\n",
       " 0.6520841121673584,\n",
       " 0.7305445075035095,\n",
       " -1.3529653549194336,\n",
       " -0.9717037081718445,\n",
       " 1.7054840326309204,\n",
       " -1.5486564636230469,\n",
       " -0.235256165266037,\n",
       " -0.4457480013370514,\n",
       " 1.197134256362915,\n",
       " -1.5917860269546509,\n",
       " 0.15508657693862915,\n",
       " -0.823511004447937,\n",
       " -0.7616699934005737,\n",
       " 0.9819682240486145,\n",
       " 0.5224264860153198,\n",
       " -1.1714351177215576,\n",
       " 0.12547320127487183,\n",
       " -0.830631673336029,\n",
       " 0.35904166102409363,\n",
       " 0.4092960059642792,\n",
       " 0.11013998091220856,\n",
       " -0.3923186659812927,\n",
       " 0.6908247470855713,\n",
       " -0.8879976868629456,\n",
       " 2.080007791519165,\n",
       " -0.02684342861175537,\n",
       " -0.5122135281562805,\n",
       " -0.2944272756576538,\n",
       " 0.3251071274280548,\n",
       " 0.7483182549476624,\n",
       " 0.7856305241584778,\n",
       " -0.05311012268066406,\n",
       " -0.0007394652348011732,\n",
       " -0.37494540214538574,\n",
       " 0.40899524092674255,\n",
       " 0.8330616354942322,\n",
       " -1.378562092781067,\n",
       " 0.8932017087936401,\n",
       " 1.2973577976226807,\n",
       " 0.6460286974906921,\n",
       " -1.4638017416000366,\n",
       " 0.276449054479599,\n",
       " -0.3286500871181488,\n",
       " 1.5042400360107422,\n",
       " -0.07776203751564026,\n",
       " 0.32283973693847656,\n",
       " -0.6937257647514343,\n",
       " -2.5114846229553223,\n",
       " 0.1421770602464676,\n",
       " 0.23313874006271362,\n",
       " 0.1463947892189026,\n",
       " 0.15449988842010498,\n",
       " 0.42311593890190125,\n",
       " 0.01193920150399208,\n",
       " -1.7394111156463623,\n",
       " 0.6277946829795837,\n",
       " -1.265967607498169,\n",
       " -1.600712776184082,\n",
       " -0.6023654341697693,\n",
       " 0.14564408361911774,\n",
       " -0.23285575211048126,\n",
       " 0.6332730650901794,\n",
       " -2.4138777256011963,\n",
       " -0.8902080059051514,\n",
       " -0.4232388734817505,\n",
       " -1.174330711364746,\n",
       " -1.044181227684021,\n",
       " 0.9759663343429565,\n",
       " -0.2668333351612091,\n",
       " -0.7141233682632446,\n",
       " 0.9408959746360779,\n",
       " -0.6314800977706909,\n",
       " -0.989881694316864,\n",
       " 0.5853299498558044,\n",
       " 0.07869531214237213,\n",
       " -1.1787700653076172,\n",
       " 1.5226374864578247,\n",
       " -0.21169331669807434,\n",
       " 0.4418257772922516,\n",
       " 0.10706347972154617,\n",
       " -2.6404101848602295,\n",
       " -0.6035224199295044,\n",
       " -1.221932053565979,\n",
       " 0.5366429090499878,\n",
       " -0.44573521614074707,\n",
       " -0.550792396068573,\n",
       " 1.5863902568817139,\n",
       " 0.4371997117996216,\n",
       " -0.10213650017976761,\n",
       " -0.7868763208389282,\n",
       " -1.2254247665405273,\n",
       " -0.8619860410690308,\n",
       " -1.3464457988739014,\n",
       " -0.4187459647655487,\n",
       " -0.026589207351207733,\n",
       " 0.6797764301300049,\n",
       " 2.1135363578796387,\n",
       " 0.3917582035064697,\n",
       " -1.140906810760498,\n",
       " -0.9409835338592529,\n",
       " -0.48592236638069153,\n",
       " -1.8853089809417725,\n",
       " 0.02050836570560932,\n",
       " -0.34549155831336975,\n",
       " -0.061567917466163635,\n",
       " 0.38305118680000305,\n",
       " -1.3410886526107788,\n",
       " 0.530795693397522,\n",
       " -1.2611923217773438,\n",
       " -0.5489796996116638,\n",
       " 0.3373396694660187,\n",
       " 0.6864591836929321,\n",
       " -3.092567205429077,\n",
       " -0.4156153202056885,\n",
       " -1.5931541919708252,\n",
       " -1.2091630697250366,\n",
       " -0.8782164454460144,\n",
       " -1.2713813781738281,\n",
       " -0.8481049537658691,\n",
       " 1.1024848222732544,\n",
       " 0.043239861726760864,\n",
       " 1.22940194606781,\n",
       " -1.4041427373886108,\n",
       " 1.1573842763900757,\n",
       " 0.9284580945968628,\n",
       " -0.2964175045490265,\n",
       " 0.762568473815918,\n",
       " -1.4051278829574585,\n",
       " 0.4226340055465698,\n",
       " 0.3613322079181671,\n",
       " 0.3760358393192291,\n",
       " 0.7056664824485779,\n",
       " 1.2975901365280151,\n",
       " -1.5649032592773438,\n",
       " 0.6577432155609131,\n",
       " -0.7221683263778687,\n",
       " -2.1344223022460938,\n",
       " -0.49357736110687256,\n",
       " 0.46393531560897827,\n",
       " -0.7122446298599243,\n",
       " 0.5299233198165894,\n",
       " 0.6264866590499878,\n",
       " 0.8038269281387329,\n",
       " 1.8654556274414062,\n",
       " 0.16882848739624023,\n",
       " -0.9901918172836304,\n",
       " -0.4779660105705261,\n",
       " 0.6871572732925415,\n",
       " -0.6448059678077698,\n",
       " -0.4173281490802765,\n",
       " -0.6141228079795837,\n",
       " -1.3427174091339111,\n",
       " -0.09184104204177856,\n",
       " -0.5662362575531006,\n",
       " 0.12227125465869904,\n",
       " -0.1422288566827774,\n",
       " -1.1426204442977905,\n",
       " 1.3592599630355835,\n",
       " 0.004203089512884617,\n",
       " -0.12296823412179947,\n",
       " -0.01944439485669136,\n",
       " 0.8795115947723389,\n",
       " 0.6837831139564514,\n",
       " -1.0289175510406494,\n",
       " 0.31184089183807373,\n",
       " 0.2895449101924896,\n",
       " 1.565527081489563,\n",
       " 0.26461100578308105,\n",
       " 0.023156309500336647,\n",
       " 0.5818381905555725,\n",
       " 0.7105114459991455,\n",
       " 0.27944543957710266,\n",
       " -0.7099173665046692,\n",
       " -0.6215237379074097,\n",
       " 1.1598893404006958,\n",
       " 0.4334432780742645,\n",
       " -0.16207663714885712,\n",
       " -0.535945475101471,\n",
       " -0.28521373867988586,\n",
       " 0.05440161004662514,\n",
       " 0.052154894918203354,\n",
       " -1.2773557901382446,\n",
       " 0.3709052801132202,\n",
       " 1.2060884237289429,\n",
       " 0.08896976709365845,\n",
       " -0.004396174103021622,\n",
       " -1.5389404296875,\n",
       " -0.6249812841415405,\n",
       " 1.1723570823669434,\n",
       " -0.9839873909950256,\n",
       " -0.3231537640094757,\n",
       " -1.4117720127105713,\n",
       " 1.2462531328201294,\n",
       " 0.8154745101928711,\n",
       " 0.39605626463890076,\n",
       " 1.4570077657699585,\n",
       " 0.5055677890777588,\n",
       " 0.4125659763813019,\n",
       " -0.29477789998054504,\n",
       " -0.35529884696006775,\n",
       " -0.8529319763183594,\n",
       " -0.8364039063453674,\n",
       " 0.5878894925117493,\n",
       " 0.5277164578437805,\n",
       " 0.3114427626132965,\n",
       " 1.2677116394042969,\n",
       " 0.4524427652359009,\n",
       " -1.1502315998077393,\n",
       " -0.6337359547615051,\n",
       " -1.2232046127319336,\n",
       " -0.25977230072021484,\n",
       " 1.1211367845535278,\n",
       " 1.049878478050232,\n",
       " -0.4936273396015167,\n",
       " 1.602941632270813,\n",
       " 0.19614022970199585,\n",
       " -0.6825623512268066,\n",
       " -1.6766774654388428,\n",
       " 1.131700873374939,\n",
       " -0.3148448169231415,\n",
       " 1.509364128112793,\n",
       " 1.259263277053833,\n",
       " 0.6473653316497803,\n",
       " -1.156469702720642,\n",
       " 0.9339218735694885,\n",
       " -1.1585586071014404,\n",
       " 1.0870532989501953,\n",
       " -1.6688233613967896,\n",
       " 0.596994936466217,\n",
       " 1.9500813484191895,\n",
       " -0.3413378894329071,\n",
       " 0.0154006602242589,\n",
       " -0.1358393132686615,\n",
       " 1.1620157957077026,\n",
       " 1.4020031690597534,\n",
       " -0.4648996889591217,\n",
       " 0.02624521218240261,\n",
       " 1.4037182331085205,\n",
       " -0.5759159326553345,\n",
       " 0.3606005012989044,\n",
       " 0.12912589311599731,\n",
       " 1.2870842218399048,\n",
       " 0.18359634280204773,\n",
       " 1.0296289920806885,\n",
       " 0.4838573634624481,\n",
       " -0.5886119604110718,\n",
       " -1.3731598854064941,\n",
       " 0.0047364127822220325,\n",
       " -0.2746075391769409,\n",
       " 1.276551365852356,\n",
       " -0.518456220626831,\n",
       " 2.1313388347625732,\n",
       " -1.4835212230682373,\n",
       " 0.2370060384273529,\n",
       " -1.080548644065857,\n",
       " 0.9821372628211975,\n",
       " -0.33583441376686096,\n",
       " -1.311058521270752,\n",
       " -0.8542528748512268,\n",
       " -0.6516870856285095,\n",
       " -0.6342672109603882,\n",
       " 0.14304505288600922,\n",
       " -1.0282760858535767,\n",
       " 2.272167444229126,\n",
       " 1.0308769941329956,\n",
       " -0.4853876829147339,\n",
       " -0.4961906671524048,\n",
       " 0.7023943066596985,\n",
       " -0.5839277505874634,\n",
       " 0.3425840437412262,\n",
       " 1.4635677337646484,\n",
       " 1.9490059614181519,\n",
       " 0.021278323605656624,\n",
       " 0.6199572086334229,\n",
       " 0.530234694480896,\n",
       " -0.06857217103242874,\n",
       " 1.1499122381210327,\n",
       " -0.1625789850950241,\n",
       " 0.9062146544456482,\n",
       " 1.4230459928512573,\n",
       " 0.7845094203948975,\n",
       " 0.19691923260688782,\n",
       " 0.15455928444862366,\n",
       " -0.6510093212127686,\n",
       " 0.5119909048080444,\n",
       " -0.6203070282936096,\n",
       " -0.039972223341464996,\n",
       " -0.6471152305603027,\n",
       " 0.7986902594566345,\n",
       " -0.25103235244750977,\n",
       " 0.4092947840690613,\n",
       " 0.41406670212745667,\n",
       " 0.5743569731712341,\n",
       " 0.010152457281947136,\n",
       " -0.38372331857681274,\n",
       " 1.035090684890747,\n",
       " -0.5749413967132568,\n",
       " 0.927298903465271,\n",
       " -0.42418378591537476,\n",
       " -0.04669499769806862,\n",
       " -0.7333269715309143,\n",
       " 0.19391481578350067,\n",
       " -0.6103805899620056,\n",
       " 0.1018814966082573,\n",
       " 0.06196003407239914,\n",
       " 0.542629599571228,\n",
       " 0.6702126860618591,\n",
       " -0.1444975584745407,\n",
       " -0.36186158657073975,\n",
       " 0.9052765965461731,\n",
       " -0.41329532861709595,\n",
       " -0.8481784462928772,\n",
       " 0.275235652923584,\n",
       " -1.1192363500595093,\n",
       " 0.02452996000647545,\n",
       " 0.2518496513366699,\n",
       " -0.16110913455486298,\n",
       " -0.1312677562236786,\n",
       " 1.522584319114685,\n",
       " 1.9745817184448242,\n",
       " -1.0068165063858032,\n",
       " -0.2558363676071167,\n",
       " -0.7560128569602966,\n",
       " -0.22281040251255035,\n",
       " -0.6346642971038818,\n",
       " -0.8363428711891174,\n",
       " 0.19225339591503143,\n",
       " 1.0607200860977173,\n",
       " 1.2407991886138916,\n",
       " -0.4029999077320099,\n",
       " -0.49293845891952515,\n",
       " -1.0711983442306519,\n",
       " 1.2302783727645874,\n",
       " 0.39925622940063477,\n",
       " 0.14118187129497528,\n",
       " 0.3124349117279053,\n",
       " 1.8764522075653076,\n",
       " 0.8323583602905273,\n",
       " -0.026623191311955452,\n",
       " 0.42297518253326416,\n",
       " -0.3928622007369995,\n",
       " -0.6828704476356506,\n",
       " -0.3784436583518982,\n",
       " -1.5651719570159912,\n",
       " -0.8830001354217529,\n",
       " -1.164300560951233,\n",
       " -1.0208295583724976,\n",
       " -1.2671023607254028,\n",
       " 0.44420796632766724,\n",
       " 2.2283782958984375,\n",
       " -0.8291449546813965,\n",
       " -0.06695405393838882,\n",
       " 0.5355209112167358,\n",
       " 0.3869479298591614,\n",
       " -0.7959340810775757,\n",
       " -0.005597359966486692,\n",
       " -0.20978997647762299,\n",
       " 1.0219354629516602,\n",
       " 0.08307506889104843,\n",
       " 1.0014492273330688,\n",
       " 0.4302417039871216,\n",
       " -0.34308165311813354,\n",
       " 0.805329442024231,\n",
       " 1.1383965015411377,\n",
       " -1.7106053829193115,\n",
       " -0.2553858757019043,\n",
       " 0.14067697525024414,\n",
       " -2.0900766849517822,\n",
       " -0.8470309376716614,\n",
       " -1.0031830072402954,\n",
       " -2.5754027366638184,\n",
       " -0.36201271414756775,\n",
       " -0.5478100180625916,\n",
       " -0.17401675879955292,\n",
       " 0.16365325450897217,\n",
       " -0.08311235904693604,\n",
       " -0.35288774967193604,\n",
       " -1.8196916580200195,\n",
       " 0.5595637559890747,\n",
       " -0.511282205581665,\n",
       " -2.716118097305298,\n",
       " -1.3833895921707153,\n",
       " 0.5359194874763489,\n",
       " -1.0199342966079712,\n",
       " 0.3906412720680237,\n",
       " -1.4040874242782593,\n",
       " -1.756796956062317,\n",
       " 1.358166217803955,\n",
       " 0.6139233112335205,\n",
       " -1.0775986909866333,\n",
       " 0.20808888971805573,\n",
       " 0.011647910811007023,\n",
       " -0.13313652575016022,\n",
       " 0.8113077282905579,\n",
       " 3.1728127002716064,\n",
       " 1.1946744918823242,\n",
       " 0.3636627197265625,\n",
       " 0.2293730229139328,\n",
       " 1.2267528772354126,\n",
       " -0.42166489362716675,\n",
       " 0.4080711007118225,\n",
       " 0.9191315770149231,\n",
       " -0.6564674377441406,\n",
       " 0.22440917789936066,\n",
       " 0.37466174364089966,\n",
       " 1.5515705347061157,\n",
       " -0.15321974456310272,\n",
       " 0.9883798956871033,\n",
       " 1.4167652130126953,\n",
       " 0.2912174463272095,\n",
       " 0.6287244558334351,\n",
       " 1.390183448791504,\n",
       " -0.2152402400970459,\n",
       " -1.061437726020813,\n",
       " 0.1828230768442154,\n",
       " -0.6676633358001709,\n",
       " 0.2675338387489319,\n",
       " -0.0050224484875798225,\n",
       " -0.2812276780605316,\n",
       " 0.039686381816864014,\n",
       " -1.1061183214187622,\n",
       " 0.4625241160392761,\n",
       " 0.8936216235160828,\n",
       " 0.19767484068870544,\n",
       " -0.6444990038871765,\n",
       " -0.8293660283088684,\n",
       " 0.576114296913147,\n",
       " -2.043569326400757,\n",
       " 1.523872971534729,\n",
       " -0.37112700939178467,\n",
       " -1.0791189670562744,\n",
       " 0.1246035173535347,\n",
       " -0.31718626618385315,\n",
       " 0.8654884099960327,\n",
       " 0.07646305114030838,\n",
       " -0.2154218852519989,\n",
       " 0.6248562335968018,\n",
       " -1.8599238395690918,\n",
       " -0.037942007184028625,\n",
       " 0.60049968957901,\n",
       " 0.40783780813217163,\n",
       " -0.5294381976127625,\n",
       " 1.3790321350097656,\n",
       " 0.7703337669372559,\n",
       " -0.7378332614898682,\n",
       " -0.7968145608901978,\n",
       " -0.07347037643194199,\n",
       " -1.0591334104537964,\n",
       " -0.4456886947154999,\n",
       " -0.48204708099365234,\n",
       " 1.8649038076400757,\n",
       " -0.21359588205814362,\n",
       " 0.012313425540924072,\n",
       " 0.4243449866771698,\n",
       " 1.9221434593200684,\n",
       " 0.3613928258419037,\n",
       " -0.566155731678009,\n",
       " -0.2027222216129303,\n",
       " -0.5467309951782227,\n",
       " -0.12807419896125793,\n",
       " -0.9643258452415466,\n",
       " -0.8423156142234802,\n",
       " -0.6809394955635071,\n",
       " -1.6327203512191772,\n",
       " 0.035733792930841446,\n",
       " 2.1431660652160645,\n",
       " -1.6886329650878906,\n",
       " 0.939841091632843,\n",
       " 1.19810950756073,\n",
       " -0.5187159180641174,\n",
       " 1.1660953760147095,\n",
       " -0.4202146828174591,\n",
       " 0.6622774004936218,\n",
       " 0.7637424468994141,\n",
       " -1.0524598360061646,\n",
       " -0.5620952248573303,\n",
       " 1.4582290649414062,\n",
       " -0.6237367391586304,\n",
       " 1.7904813289642334,\n",
       " 0.8814622163772583,\n",
       " -0.9194708466529846,\n",
       " -0.8740071654319763,\n",
       " 0.5686818957328796,\n",
       " -0.24314774572849274,\n",
       " 0.5480689406394958,\n",
       " -0.7864933013916016,\n",
       " 1.410358190536499,\n",
       " 1.2094309329986572,\n",
       " -0.15504908561706543,\n",
       " -0.16061359643936157,\n",
       " 0.9721702337265015,\n",
       " -0.5782925486564636,\n",
       " 1.2691317796707153,\n",
       " 1.21542489528656,\n",
       " -0.5319348573684692,\n",
       " -0.8314003348350525,\n",
       " -0.21688182651996613,\n",
       " 0.36235761642456055,\n",
       " -0.7833455801010132,\n",
       " 1.3453845977783203,\n",
       " 1.2052301168441772,\n",
       " -0.7242163419723511,\n",
       " -0.23849134147167206,\n",
       " -0.7917290925979614,\n",
       " -0.7642390727996826,\n",
       " -0.9063740968704224,\n",
       " -0.26567041873931885,\n",
       " -0.7060295343399048,\n",
       " -0.5491763949394226,\n",
       " 1.0223697423934937,\n",
       " -0.8722047805786133,\n",
       " -2.0322461128234863,\n",
       " -0.4778231680393219,\n",
       " -0.7645619511604309,\n",
       " -1.3265827894210815,\n",
       " -0.6331914663314819,\n",
       " -0.29940688610076904,\n",
       " -0.7571435570716858,\n",
       " -0.9188759922981262,\n",
       " 0.40025651454925537,\n",
       " 1.2061327695846558,\n",
       " -0.8874579071998596,\n",
       " -0.28340721130371094,\n",
       " 1.4704352617263794,\n",
       " -0.109587162733078,\n",
       " 1.3623220920562744,\n",
       " 0.9892475605010986,\n",
       " 1.2468711137771606,\n",
       " -1.8085476160049438,\n",
       " -0.7832204103469849,\n",
       " 1.4148482084274292,\n",
       " 0.22424308955669403,\n",
       " -0.8048632144927979,\n",
       " -0.42423006892204285,\n",
       " 0.11703190952539444,\n",
       " -1.4102263450622559,\n",
       " 0.7888635396957397,\n",
       " 2.4572975635528564,\n",
       " 0.3071434199810028,\n",
       " 0.22898784279823303,\n",
       " -0.0582842081785202,\n",
       " 2.224846124649048,\n",
       " 1.3190711736679077,\n",
       " -2.6683590412139893,\n",
       " -0.0604754239320755,\n",
       " -0.47058728337287903,\n",
       " -1.3161674737930298,\n",
       " -0.6086865067481995,\n",
       " 0.5430470705032349,\n",
       " 1.1038379669189453,\n",
       " 0.014844031073153019,\n",
       " -0.3454301059246063,\n",
       " 0.059152595698833466,\n",
       " -0.36760804057121277,\n",
       " 0.9970363974571228,\n",
       " 0.19767546653747559,\n",
       " 0.43365758657455444,\n",
       " 0.9395943880081177,\n",
       " -0.8643332719802856,\n",
       " -0.4010477364063263,\n",
       " 0.6603341698646545,\n",
       " 0.2698817849159241,\n",
       " -1.713314414024353,\n",
       " 0.5228934288024902,\n",
       " 0.6239599585533142,\n",
       " 1.520584225654602,\n",
       " 1.7851853370666504,\n",
       " -1.6605383157730103,\n",
       " -0.19118815660476685,\n",
       " 0.8717877268791199,\n",
       " 0.7334607243537903,\n",
       " 0.40273505449295044,\n",
       " -1.3629158735275269,\n",
       " 1.5931396484375,\n",
       " 0.6083713173866272,\n",
       " 1.5165201425552368,\n",
       " 0.7144361734390259,\n",
       " -0.9987698793411255,\n",
       " -1.2336883544921875,\n",
       " -0.7939777374267578,\n",
       " -0.353711873292923,\n",
       " -0.9124574661254883,\n",
       " 0.127714604139328,\n",
       " -1.0744801759719849,\n",
       " -1.1229026317596436,\n",
       " -0.35808953642845154,\n",
       " -0.49978867173194885,\n",
       " -0.9630329608917236,\n",
       " 0.7434722781181335,\n",
       " -0.762493371963501,\n",
       " -1.4207680225372314,\n",
       " 0.08374454081058502,\n",
       " 0.6365360617637634,\n",
       " 0.012879150919616222,\n",
       " -0.14917948842048645,\n",
       " 0.46067097783088684,\n",
       " 0.8927893042564392,\n",
       " 1.034341812133789,\n",
       " -0.7504938840866089,\n",
       " 0.21027489006519318,\n",
       " -0.6144307255744934,\n",
       " 0.8626965284347534,\n",
       " -0.4235838055610657,\n",
       " -1.7687547206878662,\n",
       " -0.336473673582077,\n",
       " 0.0948343575000763,\n",
       " 2.4640896320343018,\n",
       " 0.29732343554496765,\n",
       " 3.4432249069213867,\n",
       " -0.36805617809295654,\n",
       " -1.6881864070892334,\n",
       " -0.18317250907421112,\n",
       " 0.2683272957801819,\n",
       " 2.647747755050659,\n",
       " 0.3043438792228699,\n",
       " -0.3637860119342804,\n",
       " 0.48975080251693726,\n",
       " -0.5906127691268921,\n",
       " 0.07162249088287354,\n",
       " -0.720679759979248,\n",
       " 0.03940962255001068,\n",
       " -2.80678391456604,\n",
       " 0.030111338943243027,\n",
       " 0.4502207040786743,\n",
       " 0.5050973892211914,\n",
       " 0.6029464602470398,\n",
       " -0.13442359864711761,\n",
       " -0.9481380581855774,\n",
       " -0.36735010147094727,\n",
       " -0.6091283559799194,\n",
       " -0.35321244597435,\n",
       " 0.19719892740249634,\n",
       " 0.2430514395236969,\n",
       " -0.13514061272144318,\n",
       " 0.39555466175079346,\n",
       " 2.1612751483917236,\n",
       " -0.6166696548461914,\n",
       " -0.11307643353939056,\n",
       " 0.0631822943687439,\n",
       " -1.2558579444885254,\n",
       " 1.0327082872390747,\n",
       " -0.8686425089836121,\n",
       " -0.6717149019241333,\n",
       " -0.32683321833610535,\n",
       " 1.2620750665664673,\n",
       " -0.008344718255102634,\n",
       " 0.18383456766605377,\n",
       " -0.2568409740924835,\n",
       " 0.04208333045244217,\n",
       " 1.3060314655303955,\n",
       " -0.05964753031730652,\n",
       " 1.0015777349472046,\n",
       " 2.376723289489746,\n",
       " 0.39434319734573364,\n",
       " 0.6238992214202881,\n",
       " 1.8937983512878418,\n",
       " 0.6561282277107239,\n",
       " -0.006608954630792141,\n",
       " 1.4690773487091064,\n",
       " 1.4196921586990356,\n",
       " -1.4602030515670776,\n",
       " 0.8010033965110779,\n",
       " 2.156313419342041,\n",
       " 0.9434642791748047,\n",
       " -1.1205168962478638,\n",
       " -1.680059790611267,\n",
       " 0.04863185063004494,\n",
       " 0.16020160913467407,\n",
       " -0.9018815159797668,\n",
       " -1.1756539344787598,\n",
       " -0.4465246796607971,\n",
       " 1.538238525390625,\n",
       " -1.6886661052703857,\n",
       " -0.21919308602809906,\n",
       " -0.5798446536064148,\n",
       " 0.07640276849269867,\n",
       " -0.2076965570449829,\n",
       " 0.07821471244096756,\n",
       " -0.9781433939933777,\n",
       " 0.5400927066802979,\n",
       " -0.3122377395629883,\n",
       " 1.9665741920471191,\n",
       " -0.8785228729248047,\n",
       " -2.800954818725586,\n",
       " 2.0358335971832275,\n",
       " 0.4109635353088379,\n",
       " 0.6441371440887451,\n",
       " -0.9443569183349609,\n",
       " 0.6402791142463684,\n",
       " 1.0034682750701904,\n",
       " -0.8085138201713562,\n",
       " -1.3892126083374023,\n",
       " 0.4330984652042389,\n",
       " 1.0341060161590576,\n",
       " -0.6435608267784119,\n",
       " -0.4738679528236389,\n",
       " 0.611847460269928,\n",
       " -0.08909960091114044,\n",
       " 0.059156861156225204,\n",
       " -0.49939095973968506,\n",
       " -2.106813430786133,\n",
       " -0.49864840507507324,\n",
       " -1.1337754726409912,\n",
       " -0.25755223631858826,\n",
       " -0.6215916275978088,\n",
       " -0.21221044659614563,\n",
       " 0.22956222295761108,\n",
       " -0.8934663534164429,\n",
       " 0.49980002641677856,\n",
       " 0.3153747022151947,\n",
       " -0.2790585458278656,\n",
       " 0.08198350667953491,\n",
       " 0.5397440195083618,\n",
       " 0.5605983734130859,\n",
       " 0.39198732376098633,\n",
       " 1.395262598991394,\n",
       " 0.22993001341819763,\n",
       " -1.6894580125808716,\n",
       " 0.05463855713605881,\n",
       " -1.9166814088821411,\n",
       " -0.6904474496841431,\n",
       " 0.1422947496175766,\n",
       " 0.7455561757087708,\n",
       " 1.1524499654769897,\n",
       " -2.499469757080078,\n",
       " 0.05955857038497925,\n",
       " -0.5997176170349121,\n",
       " 0.26836928725242615,\n",
       " 0.2977847158908844,\n",
       " 0.8335005640983582,\n",
       " -1.966925024986267,\n",
       " -0.8206403851509094,\n",
       " -1.4131584167480469,\n",
       " -0.805535614490509,\n",
       " 0.93852698802948,\n",
       " -0.2836821377277374,\n",
       " 0.9599984884262085,\n",
       " 1.1085681915283203,\n",
       " -0.38182008266448975,\n",
       " 0.8604623675346375,\n",
       " 2.2320995330810547,\n",
       " 0.4311310946941376,\n",
       " 2.4942331314086914,\n",
       " -0.6555315256118774,\n",
       " 1.1020705699920654,\n",
       " 0.5803361535072327,\n",
       " -0.055719904601573944,\n",
       " 1.6947565078735352,\n",
       " -0.8351598381996155,\n",
       " 0.15816901624202728,\n",
       " -1.1251623630523682,\n",
       " -0.43283024430274963,\n",
       " -1.065780520439148,\n",
       " -1.3906223773956299,\n",
       " -0.5159457921981812,\n",
       " 0.2682120203971863,\n",
       " 0.17449942231178284,\n",
       " 0.3124919831752777,\n",
       " 1.2119994163513184,\n",
       " -0.07763735949993134,\n",
       " 0.9123253226280212,\n",
       " -0.8297231197357178,\n",
       " -0.9079585075378418,\n",
       " 0.045277394354343414,\n",
       " 1.2492215633392334,\n",
       " -1.5218318700790405,\n",
       " -0.9261230230331421,\n",
       " 0.304417222738266,\n",
       " -0.2636634111404419,\n",
       " 0.5313140749931335,\n",
       " 1.1017597913742065,\n",
       " -0.5530582666397095,\n",
       " 0.2514355480670929,\n",
       " -0.08802899718284607,\n",
       " -0.05750330910086632,\n",
       " -0.6768076419830322,\n",
       " -0.5149140954017639,\n",
       " 1.8220597505569458,\n",
       " -0.3936772048473358,\n",
       " -1.8169695138931274,\n",
       " -2.252598524093628,\n",
       " -0.04549383744597435,\n",
       " -0.8249303698539734,\n",
       " -0.7223578691482544,\n",
       " -0.24824842810630798,\n",
       " 0.9963988065719604,\n",
       " 1.6875765323638916,\n",
       " -0.219781756401062,\n",
       " -2.1262454986572266,\n",
       " -1.7795593738555908,\n",
       " 1.829951524734497,\n",
       " -0.04485804960131645,\n",
       " 0.6589735746383667,\n",
       " -0.3194516897201538,\n",
       " 1.319545030593872,\n",
       " 0.048815131187438965,\n",
       " 0.8224523067474365,\n",
       " -1.7553021907806396,\n",
       " 0.45932677388191223,\n",
       " -0.5063519477844238,\n",
       " 0.5467286705970764,\n",
       " 0.2249678373336792,\n",
       " -1.2651023864746094,\n",
       " -0.5861664414405823,\n",
       " 0.3951601982116699,\n",
       " -0.3647022247314453,\n",
       " 2.08583664894104,\n",
       " -0.1182112768292427,\n",
       " 1.336678147315979,\n",
       " 0.2154971808195114,\n",
       " -0.0049570719711482525,\n",
       " -0.40243035554885864,\n",
       " 1.9097747802734375,\n",
       " 0.704045832157135,\n",
       " -1.4191484451293945,\n",
       " 0.6918448805809021,\n",
       " ...]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_full_embeddings[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65590420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
